{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80189736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86134\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    " \n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    " \n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    " \n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    " \n",
    "rcParams['figure.figsize'] = 12, 8\n",
    " \n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c894552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/86134/Desktop/毕业设计/data/SemEval2014/Laptop_Train_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c3896b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2358, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2d4d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2358 entries, 0 to 2357\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          2358 non-null   int64 \n",
      " 1   content     2358 non-null   object\n",
      " 2   AspectTerm  2358 non-null   object\n",
      " 3   polarity    2358 non-null   object\n",
      " 4   from        2358 non-null   int64 \n",
      " 5   to          2358 non-null   int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 110.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "360e70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(polarity):\n",
    "    polarity = str(polarity)\n",
    "    if polarity == 'negative':\n",
    "        return 0\n",
    "    elif polarity == 'neutral':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    " \n",
    "df['sentiment'] = df.polarity.apply(to_sentiment)\n",
    " \n",
    "class_names = ['negative', 'neutral', 'positive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24583d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME= 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b85c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27345cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551dd280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    " \n",
    "    def __init__(self, reviews, review_aspects, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.review_aspects = review_aspects\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    " \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    " \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    " \n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09aa5667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2122, 7), (118, 7), (118, 7))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "df_val, df_test = train_test_split(\n",
    "    df_test,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    " \n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "525aa5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.content.to_numpy(),\n",
    "        review_aspects = df.AspectTerm.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    " \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )\n",
    " \n",
    "BATCH_SIZE = 4\n",
    " \n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2b921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\86134\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf1f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 160])\n",
      "torch.Size([4, 160])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc3f4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 18821,  1766,   118,  1332,  3402,  1106,  1139,   124,  1989,\n",
       "          1385,  4503,  1349,  2596,  1175,   146,  3742,  2541,  1251,  3719,\n",
       "          1107,  2099,   113,   123,   119,   124,   144,  3048,  1584,   191,\n",
       "           120,   188,   123,   119,   125,   144,  3048,  1584,   114,   119,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,   146,  1329,   178,  7880, 12355,  1155,  1103,  1159,   117,\n",
       "          1134,  1110,   170,  1632,  1788,  1111,  2256,  1150,  1110,  1154,\n",
       "          6427,   118,  6135,  1116,  1105,  8724, 11609,   119,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1337,   112,   188,  1304,  1936,   117,  1133,  1290,  1152,\n",
       "          1274,   112,   189,  1294,  5647,   161,  2101,  7016,  1111,  1103,\n",
       "          1839,  3621,  1107,  1142,  3395,   117,   146,  1108,  5342,  1235,\n",
       "          5647,   128,  1338,  1149,   119,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1109,  1171, 12888,  6631,  1132,  7310,  1165,  1128,  1132,\n",
       "          1684,  1107,  1103,  1843,   119,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d4c25ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f2ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    " \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    " \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output,_,_ = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a78006d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbb533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = data['input_ids'].to(device)\n",
    "#attention_mask = data['attention_mask'].to(device)\n",
    " \n",
    "#print(input_ids.shape) # batch size x seq length\n",
    "#print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f006e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.functional.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70983878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86134\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    " \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    " \n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    " \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d27ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    n_examples\n",
    "  ):\n",
    "    model = model.train()\n",
    " \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    " \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        #input_ids = input_ids.squeeze(0)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        #attention_mask = attention_mask.squeeze(0)\n",
    "        #print(input_ids.shape) # batch size x seq length\n",
    "        #print(attention_mask.shape) # batch size x seq length\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    " \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    " \n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    " \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80fd7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    " \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    " \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    " \n",
    "            loss = loss_fn(outputs, targets)\n",
    " \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    " \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a15680c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.6508777379689706 accuracy 0.8067860508953817\n",
      "Val   loss 1.0101994916486243 accuracy 0.7542372881355932\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.5189696955034183 accuracy 0.8722902921771913\n",
      "Val   loss 1.0726918102552494 accuracy 0.771186440677966\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.4460214504078925 accuracy 0.8897266729500471\n",
      "Val   loss 1.0825671553844587 accuracy 0.7796610169491526\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.40802342781692696 accuracy 0.8977379830348727\n",
      "Val   loss 1.2435544065471429 accuracy 0.7796610169491526\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.3780008438530004 accuracy 0.9010367577756833\n",
      "Val   loss 1.2879011541449776 accuracy 0.7542372881355932\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.33182521123967873 accuracy 0.9071630537229028\n",
      "Val   loss 1.3467640131595544 accuracy 0.788135593220339\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.31290070602574727 accuracy 0.9104618284637135\n",
      "Val   loss 1.301984897160825 accuracy 0.788135593220339\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.2857460824758684 accuracy 0.911875589066918\n",
      "Val   loss 1.3221986258014415 accuracy 0.788135593220339\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.27186922170682204 accuracy 0.9180018850141376\n",
      "Val   loss 1.3410102397397472 accuracy 0.8050847457627118\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.2582707756443174 accuracy 0.9175306314797361\n",
      "Val   loss 1.3410102397397472 accuracy 0.8050847457627118\n",
      "\n",
      "CPU times: total: 54min 51s\n",
      "Wall time: 56min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    " \n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    " \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    " \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    " \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    " \n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    " \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    " \n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f370c8c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 160])\n",
      "torch.Size([4, 160])\n"
     ]
    }
   ],
   "source": [
    "##没有什么用的分割线\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    " \n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34561674",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_acc.item()\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc.item()\u001b[39m\u001b[38;5;124m'\u001b[39m],[\u001b[38;5;241m1\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining history\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 670,
       "width": 1004
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(history['train_acc.item()'],[1], label='train accuracy')\n",
    "plt.plot(history['val_acc.item()'],[1], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);\n",
    "plt.xlim([0,10]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5ac37e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.3974, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.5767, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.6731, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7364, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7923, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.8317, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.8639, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.8850, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9031, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.9123, device='cuda:0', dtype=torch.float64)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['train_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c44972c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.5248, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.6468, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.6709, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7255, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7357, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7637, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7662, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.7903, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.8030, device='cuda:0', dtype=torch.float64),\n",
       " tensor(0.8043, device='cuda:0', dtype=torch.float64)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d09c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#调用训练好的模型\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18548b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8185279187817258"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09be1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    " \n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    " \n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    " \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    " \n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    " \n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ec6652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32d66e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I used to use Habitica, and I must say this is a great step up. I'd\n",
      "like to see more social features, such as sharing tasks - only one\n",
      "person has to perform said task for it to be checked off, but only\n",
      "giving that person the experience and gold. Otherwise, the price for\n",
      "subscription is too steep, thus resulting in a sub-perfect score. I\n",
      "could easily justify $0.99/month or eternal subscription for $15. If\n",
      "that price could be met, as well as fine tuning, this would be easily\n",
      "worth 5 stars.\n",
      "\n",
      "True sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    " \n",
    "review_text = y_review_texts[idx]\n",
    "true_sentiment = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "    'class_names': class_names,\n",
    "    'values': y_pred_probs[idx]\n",
    "})\n",
    " \n",
    "print(\"\\n\".join(wrap(review_text)))\n",
    "print()\n",
    "print(f'True sentiment: {class_names[true_sentiment]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e599f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ee482ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = \"Hate you!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad8856c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86134\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd4a26d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Hate you!!!\n",
      "Sentiment  : negative\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    " \n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    " \n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced400d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_GPU",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
